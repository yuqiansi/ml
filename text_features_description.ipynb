{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP30027 Machine Learning Project 2\n",
    "\n",
    "## Description of text features\n",
    "\n",
    "This notebook describes the pre-computed text features provided for Project 2. **You do not need to recompute the features yourself for this assignment** -- this information is just for your reference. However, feel free to experiment with different text features if you are interested. If you do want to try generating your own text features, some things to keep in mind:\n",
    "- There are many different decisions you can make throughout the feature design process, from the text preprocessing to the size of the output vectors. There's no guarantee that the defaults we chose will produce the best possible text features for this classification task, so feel free to experiment with different settings.\n",
    "- These features must be trained using a training corpus. Generally, the training corpus should not include validation samples, but for the purposes of this assignment we have used the entire non-test set (training+validation) as the training corpus, to allow you to experiment with different validation sets. If you recompute the text features as part of your own model, you should exclude validation samples and compute them on training samples only. For example, if you do N-fold cross-validation, this means generating N sets of features for N different training-validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# read text\n",
    "# for DEMONSTRATION PURPOSES, the entire training set will be used to train the models and also as a test set\n",
    "x_train_original = pd.read_csv(r\"book_rating_train.csv\", index_col = False, delimiter = ',', header=0)\n",
    "# use recipe name as an example\n",
    "train_corpus_name = x_train_original['Name']\n",
    "test_name = x_train_original['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Authors</th>\n",
       "      <th>PublishYear</th>\n",
       "      <th>PublishMonth</th>\n",
       "      <th>PublishDay</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Language</th>\n",
       "      <th>pagesNumber</th>\n",
       "      <th>Description</th>\n",
       "      <th>rating_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best of Dr Jean: Reading &amp; Writing</td>\n",
       "      <td>Jean R. Feldman</td>\n",
       "      <td>2005</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Teaching Resources</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>Teachers will turn to this treasury of ideas a...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Here All Dwell Free</td>\n",
       "      <td>Gertrud Mueller Nelson</td>\n",
       "      <td>1991</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>DoubleDay</td>\n",
       "      <td>NaN</td>\n",
       "      <td>364</td>\n",
       "      <td>Every human being lives a fairy tale -- an unc...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Boomer's Big Surprise</td>\n",
       "      <td>Constance W. McGeorge</td>\n",
       "      <td>2005</td>\n",
       "      <td>3</td>\n",
       "      <td>31</td>\n",
       "      <td>Chronicle Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32</td>\n",
       "      <td>&lt;i&gt;Boomer's Big Surprise&lt;/i&gt; will have special...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'll Go and Do More: Annie Dodge Wauneka, Nava...</td>\n",
       "      <td>Carolyn Niethammer</td>\n",
       "      <td>2004</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Bison Books</td>\n",
       "      <td>NaN</td>\n",
       "      <td>293</td>\n",
       "      <td>&lt;i&gt;I'll Go and Do More&lt;/i&gt; is the story of Ann...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Us</td>\n",
       "      <td>Richard       Mason</td>\n",
       "      <td>2005</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>Penguin Books Ltd</td>\n",
       "      <td>eng</td>\n",
       "      <td>352</td>\n",
       "      <td>Since their days at Oxford, they've gone their...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name                 Authors  \\\n",
       "0                 Best of Dr Jean: Reading & Writing         Jean R. Feldman   \n",
       "1                                Here All Dwell Free  Gertrud Mueller Nelson   \n",
       "2                              Boomer's Big Surprise   Constance W. McGeorge   \n",
       "3  I'll Go and Do More: Annie Dodge Wauneka, Nava...      Carolyn Niethammer   \n",
       "4                                                 Us     Richard       Mason   \n",
       "\n",
       "   PublishYear  PublishMonth  PublishDay           Publisher Language  \\\n",
       "0         2005             6           1  Teaching Resources      NaN   \n",
       "1         1991            10           1           DoubleDay      NaN   \n",
       "2         2005             3          31     Chronicle Books      NaN   \n",
       "3         2004             9           1         Bison Books      NaN   \n",
       "4         2005             7           7   Penguin Books Ltd      eng   \n",
       "\n",
       "   pagesNumber                                        Description  \\\n",
       "0           48  Teachers will turn to this treasury of ideas a...   \n",
       "1          364  Every human being lives a fairy tale -- an unc...   \n",
       "2           32  <i>Boomer's Big Surprise</i> will have special...   \n",
       "3          293  <i>I'll Go and Do More</i> is the story of Ann...   \n",
       "4          352  Since their days at Oxford, they've gone their...   \n",
       "\n",
       "   rating_label  \n",
       "0           4.0  \n",
       "1           4.0  \n",
       "2           4.0  \n",
       "3           4.0  \n",
       "4           3.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_original.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer\n",
    "\n",
    "A count vectorizer converts documents to vectors which represent word counts. Each column in the output represents a different word and the values indicate the number of times that word appeared in the document. The overall size of a count vector matrix can be quite large (the number of columns is the total number of different words used across all documents in a corpus), but most entries in the matrix are zero (each document contains only a few of all the possible words). Therefore, it is most efficient to represent the count vectors as a sparse matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_1612\\2489920220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# preprocess text and compute counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mvocab_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_corpus_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# preprocess text and compute counts\n",
    "\n",
    "vocab_name = CountVectorizer(stop_words='english').fit(train_corpus_name)\n",
    "\n",
    "# generate counts for a new set of documents\n",
    "x_train_name = vocab_name.transform(train_corpus_name)\n",
    "x_test_name = vocab_name.transform(test_name)\n",
    "\n",
    "# check the number of words in vocabulary\n",
    "print(len(vocab_name.vocabulary_))\n",
    "# check the shape of sparse matrix\n",
    "print(x_train_name.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## doc2vec\n",
    "\n",
    "doc2vec methods are an extension of word2vec. word2vec maps words to a high-dimensional vector space in such a way that words which appear in similar contexts will be close together in the space. doc2vec does a similar embedding for multi-word passages. The doc2vec (or Paragraph Vector) method was introduced by:\n",
    "\n",
    "**Le & Mikolov (2014)** Distributed Representations of Sentences and Documents<br>\n",
    "https://arxiv.org/pdf/1405.4053v2.pdf\n",
    "\n",
    "The implementation of doc2vec used for this project is from gensim and documented here:<br>\n",
    "https://radimrehurek.com/gensim/models/doc2vec.html\n",
    "\n",
    "The size of the output vector is a free parameter. Most implemementations use around 100-300 dimensions, but the best size depends on the problem you're trying to solve with the embeddings and the number of training samples, so you may wish to try different vector sizes. We provided doc2vec features for Name (vec_size = 100), Authors (vec_size = 20) and Description (vec_size = 100). The vectors themselves represent directions in a high-dimensional concept space; the columns do not represent specific words or phrases. Values in the vector are continuous real numbers and can be negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23063, 100)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "# size of the output vector\n",
    "vec_size = 100\n",
    "\n",
    "# function to preprocess and tokenize text\n",
    "def tokenize_corpus(txt, tokens_only=False):\n",
    "    for i, line in enumerate(txt):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "# tokenize a training corpus\n",
    "corpus_name = list(tokenize_corpus(train_corpus_name))\n",
    "\n",
    "# train doc2vec on the training corpus\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=vec_size, min_count=2, epochs=40)\n",
    "model.build_vocab(corpus_name)\n",
    "model.train(corpus_name, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# tokenize new documents\n",
    "doc = list(tokenize_corpus(test_name, tokens_only=True))\n",
    "\n",
    "# generate embeddings for the new documents\n",
    "x_test_name = np.zeros((len(doc),vec_size))\n",
    "for i in range(len(doc)):\n",
    "    x_test_name[i,:] = model.infer_vector(doc[i])\n",
    "    \n",
    "# check the shape of doc_emb\n",
    "print(x_test_name.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
